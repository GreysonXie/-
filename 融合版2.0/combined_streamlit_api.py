import streamlit as st
import json
import os
import re
import hashlib
import time
import csv
import pandas as pd
import configparser
from datetime import datetime
from typing import Dict, List, Tuple, DefaultDict
from collections import defaultdict
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from tenacity import retry, wait_exponential, stop_after_attempt

# é¢„è®¾å€¼
PRESET_TYPES = ["åœ°è´¨å·¥ç¨‹", "åœ°è´¨ç‰¹å¾", "å‹˜æµ‹æŠ€æœ¯", "è¯•éªŒ", "åœ°è´¨èµ„æ–™", "å·¥å…·", "æ ·æœ¬"]
PRESET_PREDICATES = ["åŒ…å«", "éœ€è¦", "æ¥æº", "å½±å“", "ç”¨äº", "å¾—åˆ°", "å‚è€ƒ", "è°ƒæŸ¥", "é€‚ç”¨", "åæ˜ "]
TYPE_MAP = {
    'PC': 'åœ°è´¨å·¥ç¨‹', 'GF': 'åœ°è´¨ç‰¹å¾', 'RH': 'å‹˜æµ‹æŠ€æœ¯',
    'TL': 'å·¥å…·', 'GI': 'åœ°è´¨èµ„æ–™', 'SP': 'æ ·æœ¬', 'EP': 'è¯•éªŒ'
}


# ------------------- TextSegmentor ç±» -------------------
class TextSegmentor:
    def __init__(self,provider_config):
        self.provider_config = provider_config['seg']
        self.llm = self._init_llm(self.provider_config['provider'])


    def _init_llm(self, provider):
        config = self.provider_config.get(provider, {})
        api_key = config.get('api_key', '')
        
        if provider == "kimi":
            base_url = config.get('base_url', "https://api.moonshot.cn/v1")
            return ChatOpenAI(
                api_key=api_key,
                base_url=base_url,
                model="moonshot-v1-32k",
                temperature=0.1
            )
        else:  # deepseek
            base_url = config.get('base_url', "https://api.deepseek.com/v1")
            return ChatOpenAI(
                api_key=api_key,
                base_url=base_url,
                model="deepseek-chat",
                temperature=0.5
            )

    def semantic_segment(self, text: str) -> List[str]:
        prompt = """
            è¯·ä¸¥æ ¼æŒ‰"4.x.x"æ ¼å¼çš„å°èŠ‚ç¼–å·è¿›è¡Œåˆ†æ®µï¼Œè§„åˆ™ï¼š
            1. å½“å‡ºç°"æ•°å­—.æ•°å­—.æ•°å­—"æ ¼å¼çš„ç¼–å·æ—¶ï¼ˆå¦‚4.3.1ï¼‰ï¼Œå°†è¯¥ç¼–å·ä¹‹åçš„å†…å®¹è§†ä¸ºæ–°æ®µè½
            2. æ®µè½æŒç»­åˆ°ä¸‹ä¸€ä¸ªåŒçº§ç¼–å·å‡ºç°ä¸ºæ­¢ï¼ˆå¦‚4.3.1çš„å†…å®¹æŒç»­åˆ°4.3.2ä¹‹å‰ï¼‰
            3. ä¿ç•™è‡ªç„¶æ®µè½ç»“æ„ï¼Œä¸è¦åˆå¹¶ä¸åŒå°èŠ‚å†…å®¹
            4. ä¿ç•™ç¼–å·å‰ç¼€ï¼Œè¾“å‡ºæ®µè½å†…å®¹
            
            ç‰¹æ®Šå¤„ç†è¦æ±‚ï¼š
            - æ•°å­—ç¼–å·åçš„ç©ºæ ¼/æ¢è¡Œç¬¦ç­‰æ ¼å¼å·®å¼‚ä¸å½±å“åˆ†æ®µåˆ¤æ–­
            - ä¿ç•™æ¡æ¬¾ä¸­çš„æ•°å­—ç¼–å·ï¼ˆå¦‚"1  éš§é“åº”é€‰æ‹©..."ä¸­çš„1/2/3ï¼‰

            ç¤ºä¾‹ï¼š
            åŸæ–‡ï¼š4.3.1 éš§é“ä½ç½®...åŸåˆ™ï¼š
            1 éš§é“åº”é€‰æ‹©...æœ‰åˆ©ã€‚
            2 éš§é“åº”é¿å¼€...
            å¤„ç†åï¼š
            éš§é“ä½ç½®...åŸåˆ™ï¼š
            1 éš§é“åº”é€‰æ‹©...æœ‰åˆ©ã€‚
            2 éš§é“åº”é¿å¼€...

            éœ€è¦åˆ†æ®µçš„æ–‡æœ¬ï¼š{text}
            """
        chunks = []
        for i in range(0, len(text), 3000):
            chunk = text[i:i+3000]
            response = self.llm.invoke(prompt.format(text=chunk)).content
            chunks.extend([p.strip() for p in response.split('\n\n') if p.strip()])
        return chunks

# ------------------- RelationExtractor ç±» -------------------
class RelationExtractor:
    def __init__(self,provider_config, relation_rules: str = None):
        self.segmentor = TextSegmentor(provider_config)
        self.provider_config = provider_config['extract']
        self.relations = self.parse_relation_rules(relation_rules) if relation_rules else {}
        self.api_config = {'max_workers': 6, 'retries': 3, 'delay': 0.8}
        self.validation_rules = {'predicate_whitelist': set(PRESET_PREDICATES)}
        
        # åˆå§‹åŒ–æç¤ºè¯
        self.template = """ä½œä¸ºåœ°è´¨å‹˜æ¢é‚»åŸŸçš„ä¸“å®¶ï¼Œä»ä»¥ä¸‹åœ°è´¨å‹˜æ¢é‚»åŸŸæ–‡æœ¬è”åˆæå–ä¸‰å…ƒç»„ä»¥æ„å»ºçŸ¥è¯†å›¾è°±ï¼š{text}
                        å¹¶æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
                        1. ä»æ–‡æœ¬ä¸­è¯†åˆ«å®ä½“ï¼Œå¹¶æŒ‰ç…§ç»™å®šçš„å®ä½“ç±»å‹ç»™ä»–ä»¬åˆ†ç±»ï¼š{entity_types}
                        2. æ ¹æ®ä¸Šä¸‹æ–‡ä»¥åŠç»“åˆåœ°è´¨é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†æ¨æ–­å®ä½“é—´çš„é€»è¾‘å…³ç³»
                        3. å…³ç³»ç±»å‹ä¸ºï¼š{predicate_list}ï¼Œå¦‚æœæ¨æ–­çš„å…³ç³»ä¸åœ¨ç»™å®šçš„ç±»å‹ä¸­ï¼š
                           - è‹¥ä¸ç»™å®šå…³ç³»ç±»å‹æ‰€è¡¨è¿°çš„æ„æ€ä¸€è‡´ï¼Œåˆ™æ›¿æ¢ä¸ºç»™å®šå…³ç³»ç±»å‹
                           - è‹¥ä¸ç»™å®šå…³ç³»ç±»å‹æ‰€è¡¨è¿°çš„æ„æ€ä¸ä¸€è‡´ï¼Œåˆ™å¿½ç•¥è¯¥ä¸‰å…ƒç»„
                        4.æ¯ä¸ªä¸‰å…ƒç»„å¿…é¡»åŒ…å«å®Œæ•´è¦ç´ 
                    
                        é¿å…æå–ç±»ä¼¼ä»¥ä¸‹çš„å†…å®¹ï¼š
                        1.æŠ½è±¡æ¦‚å¿µå¦‚"æœ¬è§„èŒƒç¬¬å››ç« "
                        2.æ³›æŒ‡è¡¨è¿°å¦‚"ç›¸å…³è¦æ±‚"
                        3.ä¸å±äºåœ°è´¨å‹˜æ¢é‚»åŸŸçš„å†…å®¹

                        å®ä½“ç±»å‹åŠç¤ºä¾‹ï¼š
                        åœ°è´¨å·¥ç¨‹(PC) - ç¤ºä¾‹ï¼šæ”¹å»ºé“è·¯ã€è¾¹å¡å·¥ç¨‹
                        åœ°è´¨ç‰¹å¾(GF) - ç¤ºä¾‹ï¼šç ‚åœŸå±‚ã€æ–­è£‚å¸¦
                        å‹˜æµ‹æŠ€æœ¯(RH) - ç¤ºä¾‹ï¼šåœ°è´¨é’»æ¢ã€ç‰©æ¢
                        å·¥å…·(TL) - ç¤ºä¾‹ï¼šé’»æœºã€æµ‹é‡ä»ª
                        åœ°è´¨èµ„æ–™(GI) - ç¤ºä¾‹ï¼šåœ°è´¨å‹˜å¯ŸæŠ¥å‘Šã€å‰–é¢å›¾
                        æ ·æœ¬(SP) - ç¤ºä¾‹ï¼šå²©èŠ¯æ ·æœ¬ã€åœŸæ ·
                        è¯•éªŒ(EP) - ç¤ºä¾‹ï¼šæ‰¿è½½åŠ›è¯•éªŒã€æ¸—é€è¯•éªŒ

                        è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š[åœ°è´¨é’»æ¢|GF]â†’ç”¨äºâ†’[æ–­è£‚å¸¦|GF]ï¼Œä¸è¦åŒ…å«*å·ç­‰ç‰¹æ®Šç¬¦å·ï¼Œä¸è¦æå–çº¯æ•°å­—ã€æ¯”ä¾‹å°ºç­‰æ— æ„ä¹‰å†…å®¹"""
        
        self.prompt = PromptTemplate(
            template=self.template,
            input_variables=["text"],
            partial_variables={
                "entity_types": "ã€".join(f"{v}({k})" for k, v in TYPE_MAP.items()),
                "predicate_list": "ã€".join(self.validation_rules['predicate_whitelist']) 
            }
        )
        
        # åˆå§‹åŒ–å¤§æ¨¡å‹
        self.extract_llm = self.segmentor._init_llm("deepseek")
        
        # ç¼“å­˜
        self.entity_cache = defaultdict(set)
        self.relation_cache = defaultdict(set)

    def _init_llm(self, provider):
        # ä»session_stateè·å–é…ç½®
        config = self.provider_config.get(provider, {})
        api_key = config.get('api_key', '')
        base_url = config.get('base_url', '')
        model_name = config.get('model', 'deepseek-chat')  # è·å–é€‰æ‹©çš„æ¨¡å‹åç§°
    

        if not api_key:
            st.error(f"è¯·æä¾›{provider}çš„APIå¯†é’¥")
            return None
            
        if provider == "kimi":
            base_url = base_url or "https://api.moonshot.cn/v1"
            return ChatOpenAI(
                api_key=api_key,
                base_url=base_url,
                model="moonshot-v1-32k",
                temperature=0.1
            )
        elif provider == "qwen":
            return ChatOpenAI(
                api_key=config['api_key'],
                base_url=config['base_url'],
                model=config.get('model', 'qwen-max'),
                temperature=0.3
            )

        elif provider == "doubao":
            return ChatOpenAI(
                api_key=config['api_key'],
                base_url=config['base_url'],
                model=config.get('model', 'chat'),
                temperature=0.5
            )
        
        else:  # deepseek
            base_url = base_url or "https://api.deepseek.com/v1"
            return ChatOpenAI(
                api_key=api_key,
                base_url=base_url,
                model=model_name,
                temperature=0.5
            )
    @staticmethod
    def parse_relation_rules(content: str) -> Dict[Tuple[str, str], str]:
        reader = csv.reader(content.strip().splitlines())
        headers = next(reader)[1:]
        relations = {}
        
        for row in reader:
            if len(row) < 1: continue
            target = row[0].split()[-1]
            for idx, rel in enumerate(row[1:]):
                if idx >= len(headers): break
                source = headers[idx].split()[-1]
                if rel.strip() not in ('/', ''):
                    relations[(source, target)] = rel.strip()
        return relations

    def _preprocess_text(self, text: str) -> str:
        text = re.sub(r'\d+[)ã€.]', '', text)
        return re.sub(r'ï¼›', 'ã€‚', text)

    def _extract_segment_id(self, text: str) -> Tuple[str, str]:
        patterns = [
            r'^(\d+\.\d+\.\d+)\s+',
            r'^(\d+\.\d+)\s+',
            r'^ç¬¬(\d+æ¡)\s+',
            r'^(Article \d+)\s+'
        ]
        for pattern in patterns:
            if match := re.search(pattern, text[:100]):
                return (match.group(1), text[match.end():])
        return (f'seg_{int(time.time())}', text)

    def _validate_triples(self, context: str, triples: List[Dict]) -> List[Dict]:
        return [t for t in triples if self._check_relation(context, t)]
    
    def _validate_triple_structure(self, triple: Dict) -> bool:
        required_keys = {'subject', 'subject_type', 'predicate', 'object', 'object_type'}
        return all(key in triple and bool(triple[key]) for key in required_keys)

    @retry(wait=wait_exponential(multiplier=1.5), stop=stop_after_attempt(3))
    def _safe_api_call(self, func, *args, **kwargs):
        time.sleep(self.api_config['delay'])
        try:
            result = func(*args, **kwargs)
            if "rate limit" in str(result).lower():
                raise Exception("APIé€Ÿç‡é™åˆ¶è§¦å‘")
            return result
        except Exception as e:
            if "429" in str(e):
                time.sleep(30)
            raise

    def _process_chunk(self, chunk: str) -> List[Dict[str, str]]:
        cache_key = hashlib.md5(chunk[:200].encode()).hexdigest()
        if cache_key in self.relation_cache:
            return json.loads(self.relation_cache[cache_key])

        try:
            chunk = self._preprocess_text(chunk) if chunk else ""
            formatted_prompt = self.prompt.format(text=chunk)
            response = self.extract_llm.invoke(formatted_prompt, timeout=30).content
            
            raw_response = response if hasattr(response, 'content') else str(response)
            initial_triples = self._parse_joint_results(raw_response)
            
            if not isinstance(initial_triples, list):
                initial_triples = []
                
            processed_triples = []
            for triple in initial_triples:
                if not self._validate_triple_structure(triple):
                    continue

                processed_triples.append({
                    "subject": str(triple.get("subject", "")),
                    "subject_type": str(triple.get("subject_type", "")),
                    "predicate": str(triple.get("predicate", "")),
                    "object": str(triple.get("object", "")),
                    "object_type": str(triple.get("object_type", ""))
                })
                
            valid_triples = self._validate_triples(chunk, processed_triples)
            self.relation_cache[cache_key] = json.dumps(valid_triples)
            return valid_triples

        except Exception as e:
            st.error(f"æ®µè½å¤„ç†å¤±è´¥: {str(e)}")
            return []

    def _parse_joint_results(self, raw: str) -> List[Dict]:
        triples = []  
        pattern = re.compile(
            r"\d*\.?\s*\[([^|\]]+)[\s|]*([A-Z]{2})\][\sâ†’|-]+([^â†’]+?)[\sâ†’|-]+\[([^|\]]+)[\s|]*([A-Z]{2})\]"
        )

        raw = re.sub(r'(\d+\.)\n', r'\1', raw)
        for line in raw.split('\n'):
            line = line.replace("ï¼š", ":").strip()
            if match := pattern.search(line):
                subj, subj_type, pred, obj, obj_type = match.groups()
                triples.append({
                    "subject": subj.strip(),
                    "subject_type": TYPE_MAP.get(subj_type.strip(), subj_type),
                    "predicate": pred.strip(),
                    "object": obj.strip(),
                    "object_type": TYPE_MAP.get(obj_type.strip(), obj_type),
                    "validated": False
                })
        return triples

    def _check_relation(self, context: str, triple: Dict) -> bool:
        if triple['subject_type'].split()[0] == triple['object_type'].split()[0]:
            return False
        
        if len(triple['subject']) < 2 or len(triple['object']) < 2:
            return False

        return True

    def extract(self, text: str, relation_rules: str = None) -> Dict:
        if relation_rules:
            self.relations = self.parse_relation_rules(relation_rules)
        
        chunks = self.segmentor.semantic_segment(text)
        results = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, chunk in enumerate(chunks):
            status_text.text(f"å¤„ç†æ®µè½ {idx+1}/{len(chunks)}...")
            progress_bar.progress((idx + 1) / len(chunks))
            
            segment_id, cleaned_chunk = self._extract_segment_id(chunk)
            triples = self._process_chunk(cleaned_chunk)
            
            results.append({
                "segment_id": segment_id,
                "text_content": cleaned_chunk[:1000],
                "triples": triples
            })
            
            time.sleep(self.api_config['delay'])
        
        progress_bar.empty()
        status_text.empty()
        
        return {
            "metadata": {
                "format_version": "1.1",
                "generated_at": datetime.now().isoformat(),
                "source": "åœ°è´¨æ–‡æœ¬æå–"
            },
            "total_segments": len(chunks),
            "segments": results
        }

    #æ–°æ–¹æ³•
    def update_prompts(self, new_prompts: dict):
        if "å…³ç³»æå–" in new_prompts:
            self.template = new_prompts["å…³ç³»æå–"]
            self.prompt = PromptTemplate(
                template=self.template,
                input_variables=["text"],
                partial_variables={
                    "entity_types": "ã€".join(f"{v}({k})" for k, v in TYPE_MAP.items()),
                    "predicate_list": "ã€".join(self.validation_rules['predicate_whitelist']) 
                }
            )


    def to_csv(self, data: Dict, output_dir: str = ".") -> Tuple[str, str]:
        all_triples = []
        for seg in data.get('segments', []):
            if isinstance(seg, dict) and 'triples' in seg:
                all_triples.extend([
                    {**triple, "segment_id": seg.get('segment_id', '')}
                    for triple in seg.get('triples', [])
                ])
        
        entities = {}
        for idx, triple in enumerate(all_triples, 1):
            for role in ['subject', 'object']:
                name = triple[f'{role}']
                ent_type = triple[f'{role}_type']
                if name not in entities:
                    entities[name] = {
                        'id': len(entities) + 1,
                        'name': name,
                        'entity_type': ent_type
                        
                    }

        entities_df = pd.DataFrame(entities.values())
        relations_df = pd.DataFrame([
            {
                'start_id': entities[triple['subject']]['id'],
                'end_id': entities[triple['object']]['id'],
                'type': triple['predicate']
            }
            for triple in all_triples
            if triple['subject'] in entities and triple['object'] in entities
        ])

        base_name = "geological_triples"
        entities_path = os.path.join(output_dir, f"{base_name}_entities.csv")
        relations_path = os.path.join(output_dir, f"{base_name}_relations.csv")

        entities_df.to_csv(entities_path, index=False,encoding='utf-8-sig')
        relations_df.to_csv(relations_path, index=False,encoding='utf-8-sig')
        
        return entities_path, relations_path
    
###########--------------------------------############
# ------------------- Streamlit åº”ç”¨ -------------------
##########---------------------------------############

DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1"
DEEPSEEK_MODEL = "deepseek-chat"

PRESET_PROMPTS = {
    "åˆ†æ®µè§„åˆ™": TextSegmentor.semantic_segment.__doc__,
    "å…³ç³»æå–": RelationExtractor.__init__.__doc__
}
def parse_prompt_update(response: str) -> dict:
    """è§£æAIç”Ÿæˆçš„æç¤ºè¯ä¿®æ”¹ï¼Œå¼ºåŒ–ç»“æ„åŒ¹é…"""
    patterns = [
        r"## (åˆ†æ®µè§„åˆ™|å…³ç³»æå–)[\sï¼š:]*([\s\S]+?)(?=## |$)",  # å¼ºåŒ–æ¿å—åŒ¹é…
        r"ã€(åˆ†æ®µè§„åˆ™|å…³ç³»æå–)ã€‘[\sï¼š:]*([\s\S]+?)(?=ã€|$)"
    ]
    
    matches = []
    for pattern in patterns:
        matches.extend(re.findall(pattern, response, flags=re.DOTALL))
    
    return {
        key: value.strip()  # å»é™¤å¤šä½™ä¿®é¥°è¯
        for key, value in matches
        if key in ["åˆ†æ®µè§„åˆ™", "å…³ç³»æå–"]  # ä¸¥æ ¼åŒ¹é…é¢„è®¾é”®
    }


def main():
    global PRESET_PROMPTS
    st.set_page_config(page_title="åœ°è´¨å·¥ç¨‹ä¸‰å…ƒç»„æå–ä¸æ ¡æ­£ç³»ç»Ÿ", layout="wide")
    st.title("åœ°è´¨å·¥ç¨‹ä¸‰å…ƒç»„æå–ä¸æ ¡æ­£ç³»ç»Ÿ")
    
    # åˆå§‹åŒ– session_state
    if 'extracted_data' not in st.session_state:
        st.session_state.extracted_data = None
    if 'current_page' not in st.session_state:
        st.session_state.current_page = 1
    if 'prompt_history' not in st.session_state:  # æ–°å¢çŠ¶æ€åˆå§‹åŒ–
        st.session_state.prompt_history = []
    if 'provider_config' not in st.session_state:
        st.session_state.provider_config = {
            'seg':{#åˆ†æ®µ
                'provider': 'kimi',#é»˜è®¤
                'kimi': {'api_key': '', 'base_url': 'https://api.moonshot.cn/v1', 'model': 'moonshot-v1-32k'},
                'deepseek': {'api_key': '', 'base_url': 'https://api.deepseek.com/v1','model':'deepseek-chat'},
                'qwen': {'api_key': '', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1','model':'qwen-max'},
                'doubao': {'api_key': '', 'base_url': 'https://doubao.com/api/v1/','model':'chat'}
            },
            'extract':{#ä¸‰å…ƒç»„æå–
                'provider': 'deepseek',  # é»˜è®¤æœåŠ¡å•†
                'kimi':{'api_key': '', 'base_url': 'https://api.moonshot.cn/v1', 'model': 'moonshot-v1-32k'},
                'deepseek': {'api_key': '', 'base_url': 'https://api.deepseek.com/v1','model':'deepseek-chat'},
                'qwen': {'api_key': '', 'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1','model':'qwen-max'},
                'doubao': {'api_key': '', 'base_url': 'https://doubao.com/api/v1/','model':'chat'}

            }
        }
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab_config,tab1, tab2, tab3,tab4 = st.tabs(["âš™ï¸ é…ç½®","ğŸ“ æ–‡æœ¬æå–", "âœï¸ ä¸‰å…ƒç»„æ ¡æ­£", "ğŸ’¾ å¯¼å‡ºç»“æœ","ğŸ¤– AIåŠ©æ‰‹"])
    
    with tab_config:
        st.header("APIé…ç½®")
        st.info("è¯·åœ¨æ­¤é…ç½®ç”¨äºåˆ†å¥å’Œå…³ç³»æå–çš„APIæœåŠ¡")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("åˆ†å¥æœåŠ¡")
            st.session_state.provider_config['seg']['provider'] = st.selectbox(
                "åˆ†æ®µæœåŠ¡å•†",
                options=["kimi", "qwen", "doubao","deepseek"],
                index=["kimi", "qwen", "doubao","deepseek"].index(
                    st.session_state.provider_config['seg']['provider']),
                    key = "seg_provider_select"
            )
            provider = st.session_state.provider_config['seg']['provider']
            model_options = {
                "kimi": ["moonshot-v1-128k", "moonshot-v1-32k"],
                "qwen": ["qwen-max", "qwen-plus", "qwen-turbo"],
                "doubao": ["chat", "pro", "max"],
                "deepseek": ["deepseek-chat", "deepseek-reasoner"]
            }

            current_models = model_options.get(provider, ["default-model"])

            st.session_state.provider_config['seg'][provider]['model'] = st.selectbox(
                f"{provider.upper()} æ¨¡å‹",
                options=current_models,
                index=current_models.index(
                    st.session_state.provider_config['seg'][provider].get('model', current_models[0])
                ),
                key=f"seg_{provider}_model"
            )

            st.session_state.provider_config['seg'][provider]['api_key'] = st.text_input(
                f"{provider.upper()} APIå¯†é’¥",
                value=st.session_state.provider_config['seg'][provider]['api_key'],
                type="password",
                key=f"seg_{provider}_api_key"
            )
            st.session_state.provider_config['seg'][provider]['base_url'] = st.text_input(
                f"{provider.upper()} APIåœ°å€",
                value=st.session_state.provider_config['seg'][provider]['base_url'],
                key=f"seg_{provider}_base_url"
            )
            
        with col2:
            st.subheader("å…³ç³»æå–æœåŠ¡ ")
            st.session_state.provider_config['extract']['provider'] = st.selectbox(
                "æå–æœåŠ¡å•†",
                options=["deepseek", "qwen", "doubao",'kimi'],
                index=["deepseek", "qwen", "doubao","kimi"].index(
                    st.session_state.provider_config['extract']['provider']),
                    key="extract_provider_select"
            )
            

            # åŠ¨æ€æ˜¾ç¤ºå¯¹åº”æœåŠ¡å•†çš„é…ç½®
            provider = st.session_state.provider_config['extract']['provider']
            model_options = {
                "deepseek": ["deepseek-chat", "deepseek-reasoner"],
                "qwen": ["qwen-max", "qwen-plus", "qwen-turbo"],
                "doubao": ["chat", "pro", "max"],
                "kimi": ["moonshot-v1-128k", "moonshot-v1-32k"]
            }
            current_models = model_options.get(provider, ["default-model"])
            current_model = st.session_state.provider_config['extract'][provider].get('model', current_models[0])
    
            # ç¡®ä¿å½“å‰æ¨¡å‹åœ¨é€‰é¡¹åˆ—è¡¨ä¸­
            if current_model not in current_models:
                current_model = current_models[0]
            st.session_state.provider_config['extract'][provider]['model'] = st.selectbox(
                f"{provider.upper()} æ¨¡å‹",
                options=current_models,
                index=current_models.index(current_model),
                key=f"extract_{provider}_model"
            )
            st.session_state.provider_config['extract'][provider]['api_key'] = st.text_input(
                f"{provider.upper()} APIå¯†é’¥",
                value=st.session_state.provider_config['extract'][provider]['api_key'],
                type="password",
                key=f"extract_{provider}_api_key"
            )
            st.session_state.provider_config['extract'][provider]['base_url'] = st.text_input(
                f"{provider.upper()} APIåœ°å€",
                value=st.session_state.provider_config['extract'][provider]['base_url'],
                key=f"extract_{provider}_base_url"
            )
        
        st.divider()
        st.subheader("æ™ºèƒ½åŠ©æ‰‹æœåŠ¡")
        # è·å–å½“å‰å·²é…ç½®çš„ä¸¤ä¸ªæœåŠ¡å•†
        seg_provider = st.session_state.provider_config['seg']['provider']
        extract_provider = st.session_state.provider_config['extract']['provider']
        available_providers = list({seg_provider, extract_provider})
        
        # åŠ¨æ€åˆ›å»ºé€‰é¡¹
        assistant_provider = st.selectbox(
            "é€‰æ‹©æœåŠ¡å•†ï¼ˆä»å·²é…ç½®æœåŠ¡ä¸­é€‰æ‹©ï¼‰",
            options=available_providers,
            index=0
        )
        st.session_state.assistant_provider = assistant_provider

        st.subheader("ä½¿ç”¨è¯´æ˜")
        st.markdown(
            """
        1. **Kimi**:
           - ç”¨äºå°†é•¿æ–‡æœ¬åˆ†å‰²æˆæœ‰æ„ä¹‰çš„æ®µè½
           - éœ€è¦ç”³è¯·APIå¯†é’¥ï¼š[Kimiå®˜ç½‘](https://platform.moonshot.cn/)
           - é»˜è®¤URLåœ°å€é€šå¸¸æ— éœ€ä¿®æ”¹
           
        2. **DeepSeek**:
           - ç”¨äºä»æ®µè½ä¸­æå–åœ°è´¨å·¥ç¨‹ä¸‰å…ƒç»„
           - éœ€è¦ç”³è¯·APIå¯†é’¥ï¼š[DeepSeekå®˜ç½‘](https://platform.deepseek.com/)
           - é»˜è®¤URLåœ°å€é€šå¸¸æ— éœ€ä¿®æ”¹

        3. **è±†åŒ…(Doubao)**:
           - æ”¯æŒåˆ†å¥å’Œå…³ç³»æå–æœåŠ¡
           - éœ€è¦ç”³è¯·APIå¯†é’¥ï¼š[ç«å±±å¼•æ“å¹³å°](https://www.volcengine.com/docs/6257/65000)
           - é»˜è®¤URLåœ°å€æ— éœ€æ›´æ”¹
        4. **QWEN**:
           - æ”¯æŒåˆ†å¥å’Œå…³ç³»æå–æœåŠ¡
           - éœ€è¦ç”³è¯·APIå¯†é’¥ï¼š[é˜¿é‡Œäº‘é€šä¹‰åƒé—®](https://help.aliyun.com/zh/model-studio/use-qwen-by-calling-api)
           - é»˜è®¤URLåœ°å€æ— éœ€æ›´æ”¹
        """)

    with tab1:
        st.header("åœ°è´¨æ–‡æœ¬ä¸‰å…ƒç»„æå–")
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ä¸Šä¼ æ–‡æœ¬")
            text_file = st.file_uploader("ä¸Šä¼ åœ°è´¨å·¥ç¨‹æ–‡æœ¬æ–‡ä»¶", type=["txt"])
            text_content = st.text_area("æˆ–ç›´æ¥è¾“å…¥æ–‡æœ¬å†…å®¹", height=300)
            
            st.subheader("å…³ç³»è§„åˆ™ (å¯é€‰)")
            rules_file = st.file_uploader("ä¸Šä¼ å…³ç³»è§„åˆ™CSV", type=["csv"])
        
        with col2:
            st.subheader("æå–è®¾ç½®")
            st.caption("APIé…ç½®è¯·åœ¨é…ç½®é¡µé¢è®¾ç½®")

            seg_provider = st.session_state.provider_config['seg']['provider']
            extract_provider = st.session_state.provider_config['extract']['provider']
        
            seg_configured = bool(st.session_state.provider_config['seg'][seg_provider]['api_key'])
            extract_configured = bool(st.session_state.provider_config['extract'][extract_provider]['api_key'])
            api_ready = seg_configured and extract_configured


            input_ready = text_content.strip() or text_file is not None
        
            # ç»„åˆéªŒè¯æ¡ä»¶
            button_disabled = not api_ready or not input_ready
            if not api_ready:
                st.warning("è¯·å…ˆåœ¨âš™ï¸é…ç½®é¡µé¢å®ŒæˆAPIå¯†é’¥è®¾ç½®")
            elif not input_ready:
                st.warning("è¯·ä¸Šä¼ æ–‡æœ¬æ–‡ä»¶æˆ–è¾“å…¥æ–‡æœ¬å†…å®¹")
            
            if st.button("å¼€å§‹æå–ä¸‰å…ƒç»„", 
                    use_container_width=True,
                    disabled=button_disabled):
                
                text = text_content if text_content else text_file.read().decode("utf-8")
                rules = rules_file.read().decode("utf-8") if rules_file else None
                
                extractor = RelationExtractor(
                    provider_config=st.session_state.provider_config,  # ä¼ é€’é…ç½®
                    relation_rules=rules
                )
                with st.spinner("æ­£åœ¨æå–ä¸‰å…ƒç»„ï¼Œè¯·ç¨å€™..."):
                    st.session_state.extracted_data = extractor.extract(text, rules)
                    st.session_state.current_page = 1
                    if st.session_state.extracted_data.get("segments"):
                        st.success("ä¸‰å…ƒç»„æå–å®Œæˆï¼")
                    else:
                        st.error("ä¸‰å…ƒç»„æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®å’Œè¾“å…¥æ–‡æœ¬")
        
        if st.session_state.extracted_data:
            st.subheader("æå–ç»“æœæ¦‚è§ˆ")
            st.json(st.session_state.extracted_data["metadata"], expanded=False)
            
            total_segments = st.session_state.extracted_data["total_segments"]
            total_triples = sum(len(seg["triples"]) for seg in st.session_state.extracted_data["segments"])
            
            st.metric("æ€»æ®µè½æ•°", total_segments)
            st.metric("æ€»ä¸‰å…ƒç»„æ•°", total_triples)
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ®µè½çš„ä¸‰å…ƒç»„
            if total_segments > 0:
                seg = st.session_state.extracted_data["segments"][0]
                st.subheader(f"æ®µè½ç¤ºä¾‹: {seg['segment_id']}")
                st.caption(f"æ–‡æœ¬å†…å®¹: {seg['text_content'][:200]}...")
                
                if seg["triples"]:
                    st.write("æå–çš„ä¸‰å…ƒç»„:")
                    for triple in seg["triples"]:
                        st.code(f"{triple['subject']} ({triple['subject_type']}) â†’ {triple['predicate']} â†’ {triple['object']} ({triple['object_type']})")
                else:
                    st.info("è¯¥æ®µè½æœªæå–åˆ°ä¸‰å…ƒç»„")

            if st.session_state.prompt_history:
                st.divider()
                if st.button("ğŸ”„ éªŒè¯ä¿®æ”¹æ•ˆæœ", help="ç‚¹å‡»ç¡®è®¤æ˜¯å¦æ»¡æ„å½“å‰æç¤ºè¯ä¿®æ”¹"):
                    if st.selectbox("æ˜¯å¦æ»¡æ„ç»“æœï¼Ÿ", ["æ»¡æ„", "ä¸æ»¡æ„"]) == "æ»¡æ„":
                        st.success("æç¤ºè¯æ›´æ–°å·²ä¿å­˜")
                    else:
                        # å›æ»šæç¤ºè¯
                        PRESET_PROMPTS = st.session_state.prompt_history[-1]["original"]
                        st.session_state.prompt_history.pop()
                        st.session_state.extracted_data = None
                        st.rerun()

    with tab2:
        if not st.session_state.extracted_data:
            st.info("è¯·å…ˆåœ¨'æ–‡æœ¬æå–'æ ‡ç­¾é¡µæå–ä¸‰å…ƒç»„")
            return
            
        st.header("ä¸‰å…ƒç»„æ ¡æ­£")
        data = st.session_state.extracted_data
        text_keys = [seg["segment_id"] for seg in data["segments"]]
        
        col_sidebar, col_main = st.columns([1, 3])
        
        with col_sidebar:
            st.subheader("æ®µè½é€‰æ‹©")
            selected_segment_id = st.selectbox("é€‰æ‹©æ®µè½", text_keys)
            selected_segment = next(seg for seg in data["segments"] if seg["segment_id"] == selected_segment_id)
            
            st.subheader("æ–‡æœ¬å†…å®¹")
            st.caption(selected_segment["text_content"][:500] + ("..." if len(selected_segment["text_content"]) > 500 else ""))
            
            # æ–°å¢ä¸‰å…ƒç»„
            if st.button("â• æ–°å¢ä¸‰å…ƒç»„", use_container_width=True):
                if "triples" not in selected_segment:
                    selected_segment["triples"] = []
                
                selected_segment["triples"].append({
                    "subject": "[æ–°ä¸»ä½“]",
                    "subject_type": PRESET_TYPES[0],
                    "predicate": PRESET_PREDICATES[0],
                    "object": "[æ–°å®¢ä½“]",
                    "object_type": PRESET_TYPES[0]
                })
                st.session_state.extracted_data = data
                st.rerun()
                
            # ä¸‹è½½å½“å‰çŠ¶æ€
            st.download_button(
                label="ğŸ’¾ ä¸‹è½½å½“å‰æ•°æ®",
                data=json.dumps(st.session_state.extracted_data, ensure_ascii=False, indent=2),
                file_name="geological_triples.json",
                mime="application/json"
            )
        
        with col_main:
            spo_list = selected_segment.get("triples", [])
            num_spo = len(spo_list)
            
            if num_spo == 0:
                st.info("è¯¥æ®µè½æ²¡æœ‰ä¸‰å…ƒç»„")
                return
            
            current_model = st.session_state.provider_config['extract'][
                st.session_state.provider_config['extract']['provider']
            ]['model']
            st.caption(f"å½“å‰ä½¿ç”¨æ¨¡å‹ï¼š{current_model} | é…ç½®äºâš™ï¸é¡µé¢")

            # åˆ†é¡µæ§åˆ¶
            page = st.session_state.current_page - 1
            if page >= num_spo:
                page = num_spo - 1
                st.session_state.current_page = num_spo
            
            # åˆ†é¡µå¯¼èˆª
            col_nav1, col_nav2, col_nav3 = st.columns([1, 2, 1])
            with col_nav1:
                if st.button("â¬…ï¸ ä¸Šä¸€ä¸ª") and page > 0:
                    st.session_state.current_page -= 1
                    st.rerun()
            with col_nav2:
                st.markdown(f"**ä¸‰å…ƒç»„ {page+1}/{num_spo}**", help="ä½¿ç”¨å·¦å³ç®­å¤´å¯¼èˆª")
            with col_nav3:
                if st.button("â¡ï¸ ä¸‹ä¸€ä¸ª") and page < num_spo - 1:
                    st.session_state.current_page += 1
                    st.rerun()
            
            # åˆ é™¤å½“å‰ä¸‰å…ƒç»„
            if st.button("ğŸ—‘ï¸ åˆ é™¤å½“å‰ä¸‰å…ƒç»„", type="primary"):
                del spo_list[page]
                st.session_state.extracted_data = data
                st.success("åˆ é™¤æˆåŠŸï¼")
                if page >= len(spo_list) and len(spo_list) > 0:
                    st.session_state.current_page = len(spo_list)
                st.rerun()
            
            # ç¼–è¾‘è¡¨å•
            spo = spo_list[page]
            with st.form(key="spo_form"):
                cols = st.columns(2)
                with cols[0]:
                    new_subject = st.text_input(
                        "ä¸»ä½“(Subject)", 
                        value=spo.get("subject", ""),
                        help="åœ°è´¨å®ä½“åç§°ï¼Œå¦‚'ç ‚å²©å±‚'"
                    )
                    new_predicate = st.selectbox(
                        "è°“è¯(Predicate)", 
                        PRESET_PREDICATES,
                        index=PRESET_PREDICATES.index(spo.get("predicate", PRESET_PREDICATES[0])),
                        help="é€‰æ‹©æˆ–è¾“å…¥åœ°è´¨å…³ç³»ç±»å‹"
                    )
                with cols[1]:
                    new_object = st.text_input("å®¢ä½“(Object)", value=spo.get("object", ""))
                    new_subject_type = st.selectbox(
                        "ä¸»ä½“ç±»å‹", 
                        PRESET_TYPES,
                        index=PRESET_TYPES.index(spo.get("subject_type", PRESET_TYPES[0]))
                    )
                    new_object_type = st.selectbox(
                        "å®¢ä½“ç±»å‹", 
                        PRESET_TYPES,
                        index=PRESET_TYPES.index(spo.get("object_type", PRESET_TYPES[0]))
                    )
                
                if st.form_submit_button("ğŸ’¾ ä¿å­˜ä¿®æ”¹"):
                    spo_list[page] = {
                        "subject": new_subject,
                        "subject_type": new_subject_type,
                        "predicate": new_predicate,
                        "object": new_object,
                        "object_type": new_object_type
                    }
                    st.session_state.extracted_data = data
                    st.success("ä¿®æ”¹å·²ä¿å­˜ï¼")
            
            # å½“å‰ä¸‰å…ƒç»„é¢„è§ˆ
            st.subheader("å½“å‰ä¸‰å…ƒç»„")
            st.json(spo)
    
    with tab3:
        if not st.session_state.extracted_data:
            st.info("è¯·å…ˆåœ¨'æ–‡æœ¬æå–'æ ‡ç­¾é¡µæå–ä¸‰å…ƒç»„")
            return
            
        st.header("å¯¼å‡ºç»“æœ")
        data = st.session_state.extracted_data
        
        st.subheader("JSONå¯¼å‡º")
        st.download_button(
            label="ä¸‹è½½JSONæ•°æ®",
            data=json.dumps(data, ensure_ascii=False, indent=2),
            file_name="geological_triples.json",
            mime="application/json"
        )
        
        st.subheader("CSVå¯¼å‡º")
        if st.button("ç”ŸæˆCSVæ–‡ä»¶", use_container_width=True):
            extractor = RelationExtractor(provider_config=st.session_state.provider_config)
            with st.spinner("æ­£åœ¨ç”ŸæˆCSVæ–‡ä»¶..."):
                entities_path, relations_path = extractor.to_csv(data)
                st.success("CSVæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
                
                col_csv1, col_csv2 = st.columns(2)
                with col_csv1:
                    # ä¿®å¤ç¼–ç é—®é¢˜ï¼šä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–å¹¶æŒ‡å®šUTF-8ç¼–ç 
                    with open(entities_path, "rb") as f:  # æ”¹ä¸ºäºŒè¿›åˆ¶æ¨¡å¼
                        st.download_button(
                            label="ä¸‹è½½å®ä½“è¡¨",
                            data=f,
                            file_name="entities.csv",
                            mime="text/csv",
                            key="entities_download"
                        )
                
                with col_csv2:
                    # ä¿®å¤ç¼–ç é—®é¢˜ï¼šä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–å¹¶æŒ‡å®šUTF-8ç¼–ç 
                    with open(relations_path, "rb") as f:  # æ”¹ä¸ºäºŒè¿›åˆ¶æ¨¡å¼
                        st.download_button(
                            label="ä¸‹è½½å…³ç³»è¡¨",
                            data=f,
                            file_name="relations.csv",
                            mime="text/csv",
                            key="relations_download"
                        )
        
        st.subheader("Neo4jå¯¼å…¥")
        st.code("""
//Â ğŸ’¥Â åˆ é™¤æ‰€æœ‰æ—§æ•°æ®ï¼ˆå¯é€‰ï¼‰

MATCHÂ (n)Â DETACHÂ DELETEÂ n;
//Â ğŸ“ŒÂ å¯¼å…¥èŠ‚ç‚¹æ•°æ®

LOADÂ CSVÂ WITHÂ HEADERSÂ FROMÂ 'file:///Nodes.csv'Â ASÂ row
CREATEÂ (n:EntityÂ {
Â Â id:Â toInteger(row.id),
Â Â name:Â row.name,
Â Â entity_type:Â row.entity_type
});

  

//Â æ­£ç¡®åˆ›å»ºåŠ¨æ€ç±»å‹çš„å…³ç³»
LOADÂ CSVÂ WITHÂ HEADERSÂ FROMÂ 'file:///relationship.csv'Â ASÂ row
MATCHÂ (a:EntityÂ {id:Â toInteger(row.start_id)})
MATCHÂ (b:EntityÂ {id:Â toInteger(row.end_id)})
CALLÂ apoc.create.relationship(a,Â row.type,Â {},Â b)Â YIELDÂ rel
RETURNÂ count(rel);

MATCHÂ (n)Â RETURNÂ nÂ LIMITÂ 25

MATCHÂ (n)-[r]->(m)
RETURNÂ n,Â r,Â m
LIMITÂ 100;


MATCHÂ (n:Entity)
REMOVEÂ n:`è¯•éªŒÂ EP`,Â n:`å‹˜æµ‹æŠ€æœ¯Â RH`,Â n:`åœ°è´¨å·¥ç¨‹Â PC`,Â n:`åœ°è´¨èµ„æ–™Â GI`,Â n:`åœ°è´¨ç‰¹å¾Â GF`;


MATCHÂ (n:Entity)
WITHÂ n.entity_typeÂ ASÂ et,Â collect(n)Â ASÂ batch
CALLÂ apoc.create.addLabels(batch,Â [et])Â YIELDÂ node
RETURNÂ count(node)Â ASÂ æ ‡è®°èŠ‚ç‚¹æ€»æ•°;

:style
node.EntityÂ {
Â Â size:Â 40px;
Â Â caption:Â '{name}';
Â Â color:Â grey;
}
node.`è¯•éªŒÂ EP`Â {
Â Â color:Â #e67e22;Â //Â æ©™è‰²
}
node.`å‹˜æµ‹æŠ€æœ¯Â RH`Â {
Â Â color:Â #9b59b6;Â //Â ç´«è‰²
}
node.`åœ°è´¨å·¥ç¨‹Â PC`Â {
Â Â color:Â #3498db;Â //Â è“è‰²
}
node.`åœ°è´¨èµ„æ–™Â GI`Â {
Â Â color:Â #f1c40f;Â //Â é»„è‰²
}
node.`åœ°è´¨ç‰¹å¾Â GF`Â {
Â Â color:Â #2ecc71;Â //Â ç»¿è‰²
}
relationshipÂ {
Â Â color:Â #e74c3c;
Â Â caption:Â '{type}';
}
        """)
    
    with tab4:        
        if st.session_state.extracted_data:
            with st.expander("ğŸ“Œ å½“å‰æå–å†…å®¹å‚è€ƒ", expanded=True):
                st.caption("ä»¥ä¸‹ä¸ºæœ€æ–°æå–å†…å®¹ï¼Œå¯ç”¨äºæç¤ºè¯ä¼˜åŒ–å‚è€ƒ")
                total_segments = st.session_state.extracted_data["total_segments"]
                total_triples = sum(len(seg["triples"]) for seg in st.session_state.extracted_data["segments"])
                
                cols = st.columns([1,2])
                with cols[0]:
                    st.metric("æ€»æ®µè½æ•°", total_segments)
                    st.metric("æ€»ä¸‰å…ƒç»„æ•°", total_triples)
                    
                with cols[1]:
                    sample_segment = st.session_state.extracted_data["segments"][0]
                    st.caption("ç¤ºä¾‹æ®µè½å†…å®¹ï¼ˆå‰200å­—ï¼‰:")
                    st.code(sample_segment["text_content"][:200] + "...")
                    
                    if sample_segment["triples"]:
                        st.caption("ç¤ºä¾‹ä¸‰å…ƒç»„:")
                        st.json(sample_segment["triples"][0]) 

        if not hasattr(st.session_state, 'assistant_provider') or not st.session_state.assistant_provider:
            st.error("è¯·å…ˆåœ¨é…ç½®é¡µé¢å®Œæˆåˆ†å¥å’Œå…³ç³»æå–æœåŠ¡çš„é…ç½®")
            return
        provider = st.session_state.assistant_provider
        try:
            # åˆ¤æ–­æœåŠ¡å•†å±äºåˆ†å¥è¿˜æ˜¯å…³ç³»æå–æœåŠ¡
            if provider in st.session_state.provider_config['seg']:
                config = st.session_state.provider_config['seg'][provider]
            else:
                config = st.session_state.provider_config['extract'][provider]
                
            if not config.get('api_key'):
                st.error(f"{provider} APIå¯†é’¥æœªé…ç½®ï¼Œè¯·å‰å¾€âš™ï¸é…ç½®é¡µé¢è®¾ç½®")
                return
        except KeyError as e:
            st.error(f"æœåŠ¡å•†é…ç½®é”™è¯¯: {str(e)}ï¼Œè¯·æ£€æŸ¥é…ç½®é¡µé¢")
            return

        st.header("æ™ºèƒ½æç¤ºè¯åŠ©æ‰‹")
        
        
        if "prompt_history" not in st.session_state:
            st.session_state.prompt_history = []
        # åˆå§‹åŒ–å¯¹è¯å†å²
        if "messages" not in st.session_state:
            st.session_state.messages = []
        # æ˜¾ç¤ºå†å²å¯¹è¯
        for msg in st.session_state.messages:
            st.chat_message(msg["role"]).write(msg["content"])


        # å¯¹è¯è¾“å…¥
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨å¯¹æç¤ºè¯çš„æ”¹è¿›è¦æ±‚"):
            # åˆå§‹åŒ–æ™ºèƒ½åŠ©æ‰‹
            assistant = ChatOpenAI(
            api_key=config['api_key'],
            base_url=config['base_url'],
            model=config['model'],
            temperature=0.5
        )
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºåœ°è´¨å·¥ç¨‹é¢†åŸŸçš„ä¸‰å…ƒç»„æå–æç¤ºè¯ä¼˜åŒ–åŠ©æ‰‹ã€‚å½“å‰ç³»ç»Ÿç”¨äºï¼š
            - ä»åœ°è´¨å·¥ç¨‹æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¸‰å…ƒç»„
            - å®ä½“ç±»å‹åŒ…æ‹¬ï¼š{PRESET_TYPES}
            - å…³ç³»ç±»å‹åŒ…æ‹¬ï¼š{PRESET_PREDICATES}
            
            ç°æœ‰æç¤ºè§„åˆ™å¦‚ä¸‹ï¼š
            {json.dumps(PRESET_PROMPTS, indent=2, ensure_ascii=False)}
            
            ä½ çš„ä¼˜åŒ–æ–¹å‘åº”ä¸“æ³¨äºï¼š
            1. å®Œå–„åœ°è´¨å®ä½“è¯†åˆ«è§„åˆ™ï¼ˆå¦‚å²©å±‚ç±»å‹ã€å·¥ç¨‹ç‰¹å¾ç­‰ï¼‰
            2. æ”¹è¿›åœ°è´¨å…³ç³»æå–é€»è¾‘ï¼ˆå¦‚åœ°å±‚æ¥è§¦å…³ç³»ã€å·¥ç¨‹å½±å“ç­‰ï¼‰
            3. ä¿æŒä¸ç°æœ‰å®ä½“/å…³ç³»ç±»å‹çš„å…¼å®¹æ€§
            4. é¿å…å¼•å…¥å…¶ä»–é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€é‡‘èï¼‰çš„å®ä½“æˆ–å…³ç³»
            
            è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š
            1. è§£é‡Šå¦‚ä½•ä¼˜åŒ–åœ°è´¨å®ä½“è¯†åˆ«è¾¹ç•Œ
            2. æ·»åŠ ç¬¦åˆåœ°è´¨é¢†åŸŸçš„æ–°å®ä½“ç±»å‹/å…³ç³»ç±»å‹
            3. ä¿®æ”¹ç°æœ‰æç¤ºæ¨¡æ¿ä¸­çš„åœ°è´¨ä¸“ä¸šé€»è¾‘
            4. ç”ŸæˆåŸºäºåœ°è´¨å·¥ç¨‹çŸ¥è¯†çš„ä¼˜åŒ–å»ºè®®"""
            
            # æ‰§è¡Œå¯¹è¯
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("assistant"):
                response = assistant.invoke([
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=prompt)
                ])
                st.markdown(response.content)
                st.session_state.messages.append({"role": "assistant", "content": response.content})
                col_accept, col_reject = st.columns(2)
            
            
                with col_accept:
                    if st.button("âœ… æ¥å—å»ºè®®", key="accept_btn"):
                        updated_prompts = parse_prompt_update(response.content)
                        if updated_prompts:
                            PRESET_PROMPTS.update(updated_prompts)
                            st.session_state.prompt_history.append({
                                "original": PRESET_PROMPTS.copy(),
                                "modified": updated_prompts
                            })
                            st.session_state.extracted_data = None
                            st.rerun()
                    else:
                        st.error("æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„æç¤ºè¯ä¿®æ”¹ï¼Œè¯·ç¡®è®¤æ ¼å¼ç¬¦åˆè¦æ±‚")

                with col_reject:
                    if st.button("âŒ æ‹’ç»å»ºè®®", key="reject_btn"):
                        st.session_state.messages.pop()  # ç§»é™¤æœ€åä¸€æ¡AIæ¶ˆæ¯
                        st.rerun()

            
            

if __name__ == "__main__":
    main()
